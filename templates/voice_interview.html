<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Interview - TalentCore AI</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
        <div class="container">
            <a class="navbar-brand" href="/">TalentCore AI</a>
        </div>
    </nav>

    <div class="container mt-4">
        <h2>Voice Interview</h2>
        <div class="row">
            <div class="col-md-6">
                <div class="card">
                    <div class="card-body">
                        <h5>Interview Controls</h5>
                        <div class="mb-3">
                            <button id="startBtn" class="btn btn-success">Start Recording</button>
                            <button id="stopBtn" class="btn btn-danger" disabled>Stop Recording</button>
                        </div>
                        <div class="mb-3">
                            <label for="transcriptArea" class="form-label">Live Transcript</label>
                            <textarea id="transcriptArea" class="form-control" rows="10" readonly></textarea>
                        </div>
                        <button id="analyzeBtn" class="btn btn-primary" disabled>Analyze Interview</button>
                    </div>
                </div>
            </div>
            <div class="col-md-6">
                <div id="analysis" class="d-none">
                    <h4>Interview Analysis</h4>
                    <div class="card">
                        <div class="card-body">
                            <div class="mb-3">
                                <h6>Sentiment Score: <span id="sentimentScore" class="badge bg-primary"></span></h6>
                            </div>
                            <div class="mb-3">
                                <h6>Engagement Score: <span id="engagementScore" class="badge bg-info"></span></h6>
                            </div>
                            <div class="mb-3">
                                <h6>Technical Red Flags:</h6>
                                <ul id="technicalFlags"></ul>
                            </div>
                            <div class="mb-3">
                                <h6>Summary:</h6>
                                <p id="summary"></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="analysisLoading" class="d-none text-center">
                    <div class="spinner-border" role="status"></div>
                    <p>Analyzing interview...</p>
                </div>
            </div>
        </div>
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let transcript = '';

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const analyzeBtn = document.getElementById('analyzeBtn');
        const transcriptArea = document.getElementById('transcriptArea');

        startBtn.addEventListener('click', async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    // In a real implementation, you'd send this to speech-to-text service
                    simulateTranscription();
                };
                
                mediaRecorder.start();
                startBtn.disabled = true;
                stopBtn.disabled = false;
                
                // Simulate real-time transcription
                startSimulatedTranscription();
                
            } catch (error) {
                alert('Error accessing microphone: ' + error.message);
            }
        });

        stopBtn.addEventListener('click', () => {
            mediaRecorder.stop();
            mediaRecorder.stream.getTracks().forEach(track => track.stop());
            
            startBtn.disabled = false;
            stopBtn.disabled = true;
            analyzeBtn.disabled = false;
        });

        analyzeBtn.addEventListener('click', async () => {
            if (!transcript.trim()) {
                alert('No transcript available for analysis');
                return;
            }
            
            document.getElementById('analysisLoading').classList.remove('d-none');
            document.getElementById('analysis').classList.add('d-none');
            
            try {
                const response = await fetch('/api/analyze-call', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ transcript })
                });
                
                const data = await response.json();
                
                if (data.error) {
                    alert('Error: ' + data.error);
                    return;
                }
                
                document.getElementById('sentimentScore').textContent = data.sentiment_score + '%';
                document.getElementById('engagementScore').textContent = data.engagement_score + '%';
                document.getElementById('summary').textContent = data.summary;
                
                const flagsList = document.getElementById('technicalFlags');
                flagsList.innerHTML = '';
                data.technical_flags.forEach(flag => {
                    const li = document.createElement('li');
                    li.textContent = flag;
                    flagsList.appendChild(li);
                });
                
                document.getElementById('analysis').classList.remove('d-none');
            } catch (error) {
                alert('Error: ' + error.message);
            } finally {
                document.getElementById('analysisLoading').classList.add('d-none');
            }
        });

        function startSimulatedTranscription() {
            // Simulate real-time transcription for demo purposes
            const sampleTexts = [
                "Hello, thank you for the opportunity to interview today.",
                "I have 5 years of experience in software development.",
                "My expertise includes Python, JavaScript, and cloud technologies.",
                "I've worked on several machine learning projects.",
                "I'm passionate about solving complex technical challenges."
            ];
            
            let index = 0;
            const interval = setInterval(() => {
                if (index < sampleTexts.length && !stopBtn.disabled) {
                    transcript += sampleTexts[index] + " ";
                    transcriptArea.value = transcript;
                    index++;
                } else {
                    clearInterval(interval);
                }
            }, 3000);
        }

        function simulateTranscription() {
            // This would be replaced with actual speech-to-text processing
            console.log('Audio recorded, would process with speech-to-text service');
        }
    </script>
</body>
</html>